---
layout: post
title: "Why I Left Application Development for AI Safety and Interpretability Research"
date: 2025-09-18 20:40:00 -0000
author: "Avery Yen"
---

## Part 1: The Dawn Of An AI Era

It was early 2024, just over one year since the launch of ChatGPT on November 30, 2022, when I started noticing the ground trembling below my feet.

Cognition Labs had just [announced Devin](https://cognition.ai/blog/introducing-devin), "the world's first AI software engineer." Though a lot of folks in various industries took notice immediately upon ChatGPT's release, I think it's safe to say that many in the software development community, including myself, had not paid nearly as much attention to the rise of LLMs as perhaps we should have. Devin was a wake up call for me, as it was for many of my colleagues, not because of any particular benchmarks, outcomes, or applications, but because it signaled the **dawn of an AI era** in coding.

I spent my formative years as a software engineer at Pivotal Labs in the 2010s, and by 2024 I was at Mechanical Orchard, founded by ex-Pivotal execs who had hired many of the same folks. We were tackling a unique problem: modernizing mainframe code and shipping 100% functionally equivalent modern software.

My division, the R&D squad, took notice of Devin immediately, and inevitably, we [announced Imogen](https://www.mechanical-orchard.com/insights/mechanical-orchard-ignites-major-shift-in-enterprise-it-transformation-with-imogen) on April 3, 2025, promising to be the Devin of Mainframes, and a lot of other related things we won't get into here (feel free to read all the marketing material). The irony of this release, of course, was not lost on me: we were aiming to supplant our fellow developers in the industry. And if eventually the robots were coding the next robots, would they one day walk out of the lab and [turn us all into paperclips](https://cepr.org/voxeu/columns/ai-and-paperclip-problem)?

Being an application developer had been all I'd known professionally. A lot of that was spent in Pivotal consulting for other, non-hightech companies like automakers, health insurers, and government agencies, which gave me a unique perspective on the very broad field of software product development. Regardless of the industry or vertical, you will eventually hit some point where you end up creating your Nth CRUD application feature, and you start to wonder, hasn't someone done this before? Tools borne out of this frustration have come and gone in popularity; amongst Pivots, as we called ourselves then (and later, as Mechanics), we (or at least, I) enjoyed frameworks and tools such as Ruby on Rails, Spring Boot, Elm, React/Node, and Phoenix.

The tools had gotten much more powerful by around 2020ish: there were generators that created UI abstractions on Web API abstractions on CRUD abstractions on database abstractions running on top of infrastructure abstractions. You didn't need to know what a memory register was, or how to manually garbage collect (a major sore point for some crustier engineers, but I digress), and if your app OoM'ed or failed to compute in time, you just bumped up the resources in your console and paid a slightly higher cloud bill (you're welcome, Bezos!). You could assemble a web or mobile frontend app pretty quickly by then, usually in a matter of days to launch a fully end-to-end CRUD feature or three.

Compute might have been cheap, but software developers were decidedly *not*. It's no secret that by 2024, full-time, mid-level software developers in the US nearly all earned six-figure salaries, while those on the lower ends of the seniority scale still enjoyed a high degree of marketability, portability, and desirability. The ability to code full apps from scratch was a marker of intellectual rigor and disciplined success. (Learn-to-code programs and bootcamps aimed to democratize this in the era before AI, the rise and fall of which is beyond the scope of this blog.)

The AI coding agent has all but become a meme now, but there is reason to believe that it is here to stay. I am no fortune teller, but one reason I have suspicions that robot coding is here to stay is that, **software engineers are always looking to work at higher levels of abstraction**.

Until the coding robots arrived, the highest level of abstractions I knew were those I was working on then: UI components, CRUD operation generators, database paradigms, infrastructure-as-code. All of that had changed once the coding robots came for our jobs, and I was part of the problem. Now, software development *could reasonably become* prompt engineering. (Hey, if it makes people's specs and tests better, maybe we're better off?) What an ironic time to be alive and working in software.

Now the debate is of course still out on what the *right* level of abstraction is for the coding robots and the coders to co-operate at, and frontier models and agents push that boundary every day, but I think the dust will settle eventually. Expert practitioners like my former colleague Kent Beck put forth great opinions on AI Augmented Coding very elegantly; I don't. Go read [his website](https://kentbeck.com/) and [blog Tidy First](https://tidyfirst.substack.com/) instead.
## Part 2: Push-Pull

All major life decision points have pushes and pulls, and Part 1 set the scene for the push in my life. Then came the pull.

I emerged from Mechanical Orchard in 2025 stunned, slightly jaded, and somewhat scared of what was coming next to my industry and role as a software engineer, I came to learn of the work of AI researchers working in mechanistic interpretability in deep neural networks. (In case you made it this far without knowing, Large Language Models like ChatGPT are very large, deep neural networks.) I happened to meet David Bau through a mutual connection, and learned about his work contributing to this field, having advanced techniques such as direct model editing - being able to surgically tell a model what to think or do *after* it has been trained.

AI Safety, depending on who you ask, is something between a buzzword and a moral imperative to ensure the future of humanity. It's probably still not on either extreme *yet*; fossil fuel-induced climate change will probably make the Earth uninhabitable sooner than AI, but science fiction has a funny way of turning into people's research dreams no matter how bad they are for us, so who knows?

Regardless, a real and present need exists in the world of AI: figuring out what they know (or think they know, or pretend to know, or what knowledge they model). This is what the discipline of *AI interpretability* aims to do.

The path was opening up before me, and I decided I had to walk it to see where it went. I had to become a black swan engineer.

## Part 3: Becoming A Black Swan Engineer

The rise of AI is our generation's [black swan event](https://en.wikipedia.org/wiki/Black_swan_theory): an occurrence so rare and outside expectations that no one truly predicted its speed or impact. Its consequences will be extreme, and we're still figuring out whether they'll be beneficial or catastrophic. We need black swan engineers: rare birds who recognize the rise of AI for what it is and who are charged with **ensuring the impact of AI is beneficial rather than catastrophic**. (This is, in fact, a term I just made up, but it sounded like I would have cool plumage, and Claude AI liked it too!) To get there, we need the foundations of AI engineering, an appetite for lifelong learning and research, and the desire to hold AI systems in check by cracking them open and examining their innards.

If, like me, you came to this field and are along for the journey, you might need a very quick, down-and-dirty primer on why this field of Interpretability exists and why it needs dedicated, clever people with tech knowledge. Here it is: LLM AI systems have at their very core a bunch of numbers (in the deep neural network part) that are related to each other, the inputs, and the outputs of the system. The problem is, there are basically an unknowable amount of these numbers and all of them are basically *completely made up by the computer*.

Where do these numbers come from? I encourage you to find an open course on deep learning to understand more, but in short, the computer is trying to compute a giant matrix (the model's **parameters** or **weights**). In short, an LLM is trained to **predict the next word in a sequence**. The prediction's accuracy is measured by a **loss function**, and this loss is used to automatically update the model's internal matrix through **gradient descent**, an automatic function that indicates where each parameter should be adjusted, until its predictions are consistently accurate.

Nowhere in this entire journey from zero to LLM does any human tell the computer what to do or think once training begins. Deep learning by definition creates a giant black box full of numbers where **no one, not even the computer, can say where those numbers came from or what they mean**. Interpretability exists to try to figure something out about the numbers which result from LLM training, and seeks to answer questions like what does a model *know* or *what can it reason about*. Interpretability research has led to fascinating findings like finding [topics that an LLM knows something about](https://www.khoury.northeastern.edu/khoury-researchers-find-political-censorship-in-chinese-ai-model-and-explain-how-to-get-around-it/) but finding out it says another thing.

Unlike living human brains, which our species has collectively agreed fundamentally never to crack open for the sake of science, we don't have such restrictions on LLMs. We are developing the foundations, techniques, and tools to crack open LLMs. You should join us!

If, like I was, you are puzzled about where to start after coming from an app dev background, it might help to know a couple of basic vocab words. I don't mean attention heads and vectors, though you will of course learn those, too. I mean "research scientist" vs "research engineer". I won't go too far into the differences, but fundamentally the goal of any researcher is to create new knowledge. The difference is that "scientist" roles often are primary concerned with knowledge **novelty and discovery**, whereas "engineers" are still expected to read papers in conjunction with the scientists, collaborate to gain a deep understanding of the research problem, then find **practical solutions to the technical challenges of implementing new research**.

Start with Stage 1 from [this blog post by Neel Nanda](https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher) on how to become an interpretability *researcher*, then come back once you've followed most of his curriculum. By then, you should have studied basic math, read one or two papers *max* on the architecture of LLMs, implemented a transformer from scratch, and written mech interp code. Then we will diverge slightly, so keep reading his post a little bit, but stick around here for *our* journey of combing software engineering experience and the work of an interpretability *research engineer*. Instead of opting to replicate and extend existing research, searching for research problems, and writing papers, we are going to practice **replicating, adapting, and scaling** research results and **applying them to specific domain settings**.

Of course, you can follow the rest of Neel's post to the letter, but you and I are coming to this with a very different background that *adds value to any research lab*. And while one is certainly able to pivot entirely into pure, fundamental research, perhaps by obtaining a PhD, no lab is complete without the skills to implement clean, reusable, maintainable code. And not everyone wants or needs a PhD.

See you next time!
